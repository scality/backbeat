evaluation_interval: 1m
rule_files:
  - alerts.rendered.yaml

tests:

  - name: LifecycleProducer Replicas
    interval: 1m
    input_series:
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-producer-headless"}
        values: 1 0
    alert_rule_test:
      - alertname: LifecycleProducerDown
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleProducerDown
        eval_time: 2m
        exp_alerts:
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Lifecycle producer pod has been down for 30 seconds"
              summary: "Lifecycle producer service is down"

  - name: LifecycleBucketProcessor Replicas
    interval: 1m
    input_series:
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-bucket-processor-headless",pod="bucket-1"}
        values: 1 1 1 
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-bucket-processor-headless",pod="bucket-2"}
        values: 1 1 0
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-bucket-processor-headless",pod="bucket-3"}
        values: 1 0 0
    alert_rule_test:
      - alertname: LifecycleBucketProcessorDegraded
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleBucketProcessorCritical
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleBucketProcessorDegraded
        eval_time: 2m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-bucket-processor
              description: "Less than 100% of lifecycle bucket processors are up and healthy"
              summary: "Degraded lifecycle bucket processor"
      - alertname: LifecycleBucketProcessorCritical
        eval_time: 2m
        exp_alerts: []
      - alertname: LifecycleBucketProcessorDegraded
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-bucket-processor
              description: "Less than 100% of lifecycle bucket processors are up and healthy"
              summary: "Degraded lifecycle bucket processor"
      - alertname: LifecycleBucketProcessorCritical
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-bucket-processor
              description: "Less than 50% of lifecycle bucket processors are up and healthy"
              summary: "Degraded lifecycle bucket processor"

  - name: LifecycleObjectProcessor Replicas
    interval: 1m
    input_series:
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-object-processor-headless",pod="object-1"}
        values: 1 1 1 
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-object-processor-headless",pod="object-2"}
        values: 1 1 0
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-object-processor-headless",pod="object-3"}
        values: 1 0 0
    alert_rule_test:
      - alertname: LifecycleObjectProcessorDegraded
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleObjectProcessorCritical
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleObjectProcessorDegraded
        eval_time: 2m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-object-processor
              description: "Less than 100% of lifecycle object processors for expiration are up and healthy"
              summary: "Degraded lifecycle object processor"
      - alertname: LifecycleObjectProcessorCritical
        eval_time: 2m
        exp_alerts: []
      - alertname: LifecycleObjectProcessorDegraded
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-object-processor
              description: "Less than 100% of lifecycle object processors for expiration are up and healthy"
              summary: "Degraded lifecycle object processor"
      - alertname: LifecycleObjectProcessorCritical
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-object-processor
              description: "Less than 50% of lifecycle object processors for expiration are up and healthy"
              summary: "Degraded lifecycle object processor"

  - name: LifecycleTransitionProcessor Replicas
    interval: 1m
    input_series:
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-transition-processor-headless",pod="object-1"}
        values: 1 1 1
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-transition-processor-headless",pod="object-2"}
        values: 1 1 0
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-transition-processor-headless",pod="object-3"}
        values: 1 0 0
    alert_rule_test:
      - alertname: LifecycleTransitionProcessorDegraded
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleTransitionProcessorCritical
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleTransitionProcessorDegraded
        eval_time: 2m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-transition-processor
              description: "Less than 100% of lifecycle transition processors are up and healthy"
              summary: "Degraded lifecycle transition processor"
      - alertname: LifecycleTransitionProcessorCritical
        eval_time: 2m
        exp_alerts: []
      - alertname: LifecycleTransitionProcessorDegraded
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-transition-processor
              description: "Less than 100% of lifecycle transition processors are up and healthy"
              summary: "Degraded lifecycle transition processor"
      - alertname: LifecycleTransitionProcessorCritical
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-transition-processor
              description: "Less than 50% of lifecycle transition processors are up and healthy"
              summary: "Degraded lifecycle transition processor"

  - name: LifecycleLateScanWarning
    interval: 1m
    input_series:
      - series: s3_lifecycle_latest_batch_start_time{namespace="zenko",job="artesca-data-backbeat-lifecycle-producer-headless"}
        values: 0 0 0 0 240000 299000 299000 299000
    alert_rule_test:
      - alertname: LifecycleLateScanWarning
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleLateScanWarning
        eval_time: 2m
        exp_alerts: []
      - alertname: LifecycleLateScanWarning
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScanWarning
        eval_time: 4m
        exp_alerts: []
      - alertname: LifecycleLateScanWarning
        eval_time: 5m
        exp_alerts: []
      - alertname: LifecycleLateScanWarning
        eval_time: 6m
        exp_alerts: []
      - alertname: LifecycleLateScanWarning
        eval_time: 7m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"

  - name: LifecycleLateScanCritical
    interval: 1m
    input_series:
      - series: s3_lifecycle_latest_batch_start_time{namespace="zenko",job="artesca-data-backbeat-lifecycle-producer-headless"}
        values: 0 0 0 0 0 60000 180000 240000
    alert_rule_test:
      - alertname: LifecycleLateScanCritical
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleLateScanCritical
        eval_time: 2m
        exp_alerts: []
      - alertname: LifecycleLateScanCritical
        eval_time: 3m
        exp_alerts: []
      - alertname: LifecycleLateScanCritical
        eval_time: 4m
        exp_alerts:
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 3m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScanCritical
        eval_time: 5m
        exp_alerts:
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 3m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScanCritical
        eval_time: 6m
        exp_alerts: []
      - alertname: LifecycleLateScanCritical
        eval_time: 7m
        exp_alerts: []

  - name: LifecycleLatency
    interval: 10m
    input_series:
      - series: s3_lifecycle_latency_seconds_bucket{le="10", namespace="zenko", type="transition", location="us-east-1", job="artesca-data-backbeat-lifecycle-transition-processor-headless"}
        values: 100 200 300 305 310 315 320 325 330 430
      - series: s3_lifecycle_latency_seconds_bucket{le="50", namespace="zenko", type="transition", location="us-east-1", job="artesca-data-backbeat-lifecycle-transition-processor-headless"}
        values: 100 200 300 305 310 315 320 325 330 430
      - series: s3_lifecycle_latency_seconds_bucket{le="180", namespace="zenko", type="transition", location="us-east-1", job="artesca-data-backbeat-lifecycle-transition-processor-headless"}
        values: 100 200 300 400 500 600 605 610 615 620
      - series: s3_lifecycle_latency_seconds_bucket{le="240", namespace="zenko", type="transition", location="us-east-1", job="artesca-data-backbeat-lifecycle-transition-processor-headless"}
        values: 100 200 300 400 500 600 700 800 900 1000
      - series: s3_lifecycle_latency_seconds_bucket{le="+inf", namespace="zenko", type="transition", location="us-east-1", job="artesca-data-backbeat-lifecycle-transition-processor-headless"}
        values: 100 200 300 400 500 600 700 800 900 1000
    alert_rule_test:
      - alertname: LifecycleLatencyWarning
        eval_time: 10m
        exp_alerts: []
      - alertname: LifecycleLatencyCritical
        eval_time: 10m
        exp_alerts: []
      - alertname: LifecycleLatencyWarning
        eval_time: 20m
        exp_alerts: []
      - alertname: LifecycleLatencyCritical
        eval_time: 20m
        exp_alerts: []
      - alertname: LifecycleLatencyWarning
        eval_time: 30m
        exp_alerts:
          - exp_labels:
              severity: warning
              type: transition
              location: us-east-1
            exp_annotations:
              description: "Lifecycle latency for `transition` is above the warning threshold on location `us-east-1`."
              summary: "High lifecycle latency"
      - alertname: LifecycleLatencyCritical
        eval_time: 30m
        exp_alerts: []
      - alertname: LifecycleLatencyWarning
        eval_time: 60m
        exp_alerts:
          - exp_labels:
              severity: warning
              type: transition
              location: us-east-1
            exp_annotations:
              description: "Lifecycle latency for `transition` is above the warning threshold on location `us-east-1`."
              summary: "High lifecycle latency"
      - alertname: LifecycleLatencyCritical
        eval_time: 60m
        exp_alerts:
          - exp_labels:
              severity: critical
              type: transition
              location: us-east-1
            exp_annotations:
              description: "Lifecycle latency for `transition` is above the critical threshold on location `us-east-1`."
              summary: "Very high lifecycle latency"

  - name: KafkaConsumerSlowTask
    interval: 1m
    input_series:
      - series: s3_zenko_queue_slowTasks_count{namespace="zenko",job="artesca-data-backbeat-object-processor-headless"}
        values: 0 0 0 0 1 0 0 0 0 0
      - series: s3_zenko_queue_slowTasks_count{namespace="zenko",job="artesca-data-backbeat-bucket-processor-headless"}
        values: 0 0 1 1 1 1 1 1 1 0
    alert_rule_test:
      - alertname: KafkaConsumerSlowTask
        eval_time: 1m
        exp_alerts: []
      - alertname: KafkaConsumerSlowTask
        eval_time: 3m
        exp_alerts: []
      - alertname: KafkaConsumerSlowTask
        eval_time: 5m
        exp_alerts: []
      - alertname: KafkaConsumerSlowTask
        eval_time: 6m
        exp_alerts: []
      - alertname: KafkaConsumerSlowTask
        eval_time: 7m
        exp_alerts:
          - exp_labels:
              severity: warning
              job: artesca-data-backbeat-bucket-processor-headless
            exp_annotations:
              description: >-
                Some tasks are taking too long to process in artesca-data-backbeat-bucket-processor-headless. This is not expected, and
                may be a sign that other components are not behaving nominally or may need to be scaled.

                If this alert lasts, it may mean the task is blocked, and that the consumer should be
                restarted.
              summary: Some Kafka messages are taking too long to process
      - alertname: KafkaConsumerSlowTask
        eval_time: 8m
        exp_alerts:
          - exp_labels:
              severity: warning
              job: artesca-data-backbeat-bucket-processor-headless
            exp_annotations:
              description: >-
                Some tasks are taking too long to process in artesca-data-backbeat-bucket-processor-headless. This is not expected, and
                may be a sign that other components are not behaving nominally or may need to be scaled.

                If this alert lasts, it may mean the task is blocked, and that the consumer should be
                restarted.
              summary: Some Kafka messages are taking too long to process
      - alertname: KafkaConsumerSlowTask
        eval_time: 9m
        exp_alerts: []

  - name: KafkaConsumerRebalance
    interval: 1m
    input_series:
      - series: s3_zenko_queue_rebalance_total_count{namespace="zenko",job="artesca-data-backbeat-object-processor-headless",status="drained",pod="foo"}
        values: 1 2 _ _ stale
      - series: s3_zenko_queue_rebalance_total_count{namespace="zenko",job="artesca-data-backbeat-object-processor-headless",status="timeout",pod="foo"}
        values: _ 1 _ _ stale
      - series: s3_zenko_queue_rebalance_total_count{namespace="zenko",job="artesca-data-backbeat-object-processor-headless",status="drained",pod="bar"}
        values: _ _ 1 2 3 4 5 6 7
      - series: s3_zenko_queue_rebalance_total_count{namespace="zenko",job="artesca-data-backbeat-object-processor-headless",status="timeout",pod="bar"}
        values: _ _ 0 0 0 0 0 0 0
    alert_rule_test:
      - alertname: KafkaConsumerRebalanceTimeout
        eval_time: 0m
        exp_alerts: []
      - alertname: KafkaConsumerRebalanceTimeout
        eval_time: 1m
        exp_alerts:
          - exp_labels:
              severity: critical
              pod: "foo"
            exp_annotations:
              summary: Kafka consumer has stopped consuming messages
              description: Kafka rebalance has timed out for pod `foo`, which indicates that the consumer is not working anymore, and should be restarted.
      - alertname: KafkaConsumerRebalanceTimeout
        eval_time: 2m
        exp_alerts:
          - exp_labels:
              severity: critical
              pod: "foo"
            exp_annotations:
              summary: Kafka consumer has stopped consuming messages
              description: Kafka rebalance has timed out for pod `foo`, which indicates that the consumer is not working anymore, and should be restarted.
      - alertname: KafkaConsumerRebalanceTimeout
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: critical
              pod: "foo"
            exp_annotations:
              summary: Kafka consumer has stopped consuming messages
              description: Kafka rebalance has timed out for pod `foo`, which indicates that the consumer is not working anymore, and should be restarted.
      - alertname: KafkaConsumerRebalanceTimeouts
        eval_time: 4m
        exp_alerts: []

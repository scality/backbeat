evaluation_interval: 1m
rule_files:
  - alerts.rendered.yaml

tests:

  - name: LifecycleProducer Replicas
    interval: 1m
    input_series:
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-producer-headless"}
        values: 1 0
    alert_rule_test:
      - alertname: LifecycleProducerDown
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleProducerDown
        eval_time: 2m
        exp_alerts:
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Lifecycle producer pod has been down for 30 seconds"
              summary: "Lifecycle producer service is down"

  - name: LifecycleBucketProcessor Replicas
    interval: 1m
    input_series:
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-bucket-processor-headless",pod="bucket-1"}
        values: 1 1 1 
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-bucket-processor-headless",pod="bucket-2"}
        values: 1 1 0
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-bucket-processor-headless",pod="bucket-3"}
        values: 1 0 0
    alert_rule_test:
      - alertname: LifecycleBucketProcessorDown
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleBucketProcessorDown
        eval_time: 2m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-bucket-processor
              description: "Less than 100% of lifecycle bucket processors are up and healthy"
              summary: "Degraded lifecycle bucket processor"
      - alertname: LifecycleBucketProcessorDown
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-bucket-processor
              description: "Less than 100% of lifecycle bucket processors are up and healthy"
              summary: "Degraded lifecycle bucket processor"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-bucket-processor
              description: "Less than 50% of lifecycle bucket processors are up and healthy"
              summary: "Degraded lifecycle bucket processor"

  - name: LifecycleObjectProcessor Replicas
    interval: 1m
    input_series:
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-object-processor-headless",pod="object-1"}
        values: 1 1 1 
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-object-processor-headless",pod="object-2"}
        values: 1 1 0
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-object-processor-headless",pod="object-3"}
        values: 1 0 0
    alert_rule_test:
      - alertname: LifecycleObjectProcessorDown
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleObjectProcessorDown
        eval_time: 2m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-object-processor
              description: "Less than 100% of lifecycle object processors for expiration are up and healthy"
              summary: "Degraded lifecycle object processor"
      - alertname: LifecycleObjectProcessorDown
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-object-processor
              description: "Less than 100% of lifecycle object processors for expiration are up and healthy"
              summary: "Degraded lifecycle object processor"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-object-processor
              description: "Less than 50% of lifecycle object processors for expiration are up and healthy"
              summary: "Degraded lifecycle object processor"

  - name: LifecycleBucketProcessorRequest
    interval: 1m
    input_series:
      - series: s3_lifecycle_s3_operations_total{namespace="zenko", job="artesca-data-backbeat-lifecycle-bucket-processor-headless", status="200", process="bucket"}
        values: 0+99x4 495+97x4 980+95x4 1455+98x4
      - series: s3_lifecycle_s3_operations_total{namespace="zenko", job="artesca-data-backbeat-lifecycle-bucket-processor-headless", status="413", process="bucket"}
        values: 0+1x4    5+3x4   20+5x4    45+2x4
      - series: s3_lifecycle_s3_operations_total{namespace="zenko", job="artesca-data-backbeat-lifecycle-object-processor-headless", status="200", process="expiration"}
        values: 0+100x19
    alert_rule_test:
      - alertname: LifecycleBucketProcessorRequest
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleBucketProcessorRequest
        eval_time: 5m
        exp_alerts: []
      - alertname: LifecycleBucketProcessorRequest
        eval_time: 9m
        exp_alerts: []
      - alertname: LifecycleBucketProcessorRequest
        eval_time: 10m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-bucket-processor
              description: "More than 3% of S3 requests by bucket processors resulting in errors"
              summary: "High rate of S3 request errors"
      - alertname: LifecycleBucketProcessorRequest
        eval_time: 14m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-bucket-processor
              description: "More than 3% of S3 requests by bucket processors resulting in errors"
              summary: "High rate of S3 request errors"
      - alertname: LifecycleBucketProcessorRequest
        eval_time: 15m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-bucket-processor
              description: "More than 3% of S3 requests by bucket processors resulting in errors"
              summary: "High rate of S3 request errors"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-bucket-processor
              description: "More than 5% of S3 requests by bucket processors resulting in errors"
              summary: "Very high rate of S3 request errors"
      - alertname: LifecycleBucketProcessorRequest
        eval_time: 19m
        exp_alerts: []

  - name: LifecycleObjectProcessorRequest
    interval: 1m
    input_series:
      - series: s3_lifecycle_s3_operations_total{namespace="zenko", job="artesca-data-backbeat-lifecycle-object-processor-headless", status="200", process="expiration"}
        values: 0+99x4 495+97x4 980+95x4 1455+98x4
      - series: s3_lifecycle_s3_operations_total{namespace="zenko", job="artesca-data-backbeat-lifecycle-object-processor-headless", status="413", process="expiration"}
        values: 0+1x4    5+3x4   20+5x4    45+2x4
      - series: s3_lifecycle_s3_operations_total{namespace="zenko", job="artesca-data-backbeat-lifecycle-bucket-processor-headless", status="200", process="bucket"}
        values: 0+100x19
    alert_rule_test:
      - alertname: LifecycleObjectProcessorRequest
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleObjectProcessorRequest
        eval_time: 5m
        exp_alerts: []
      - alertname: LifecycleObjectProcessorRequest
        eval_time: 9m
        exp_alerts: []
      - alertname: LifecycleObjectProcessorRequest
        eval_time: 10m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-object-processor
              description: "More than 3% of S3 requests by object processors resulting in errors"
              summary: "High rate of S3 request errors"
      - alertname: LifecycleObjectProcessorRequest
        eval_time: 14m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-object-processor
              description: "More than 3% of S3 requests by object processors resulting in errors"
              summary: "High rate of S3 request errors"
      - alertname: LifecycleObjectProcessorRequest
        eval_time: 15m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-object-processor
              description: "More than 3% of S3 requests by object processors resulting in errors"
              summary: "High rate of S3 request errors"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-object-processor
              description: "More than 5% of S3 requests by object processors resulting in errors"
              summary: "Very high rate of S3 request errors"
      - alertname: LifecycleObjectProcessorRequest
        eval_time: 19m
        exp_alerts: []

  - name: LifecycleTransitionProcessor Replicas
    interval: 1m
    input_series:
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-transition-processor-headless",pod="object-1"}
        values: 1 1 1
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-transition-processor-headless",pod="object-2"}
        values: 1 1 0
      - series: up{namespace="zenko",job="artesca-data-backbeat-lifecycle-transition-processor-headless",pod="object-3"}
        values: 1 0 0
    alert_rule_test:
      - alertname: LifecycleTransitionProcessorDown
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleTransitionProcessorDown
        eval_time: 2m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-transition-processor
              description: "Less than 100% of lifecycle transition processors are up and healthy"
              summary: "Degraded lifecycle transition processor"
      - alertname: LifecycleTransitionProcessorDown
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-transition-processor
              description: "Less than 100% of lifecycle transition processors are up and healthy"
              summary: "Degraded lifecycle transition processor"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-transition-processor
              description: "Less than 50% of lifecycle transition processors are up and healthy"
              summary: "Degraded lifecycle transition processor"

  - name: LifecycleLateScan
    interval: 1m
    input_series:
      - series: kube_service_created{namespace="zenko",service="artesca-data-backbeat-lifecycle-producer-headless",job="kube-state-metrics"}
        values: 5+0x14
      - series: s3_lifecycle_latest_batch_start_time{namespace="zenko",job="artesca-data-backbeat-lifecycle-producer-headless", pod="foo"}
        values: _    _ 0 0 0 60000 180000 240000 stale
      - series: s3_lifecycle_latest_batch_start_time{namespace="zenko",job="artesca-data-backbeat-lifecycle-producer-headless",pod="bar"}
        values: _    _ _ _ _ _     _      _      _     _ _ _ 0 600000 720000
    alert_rule_test:
      - alertname: LifecycleLateScan
        eval_time: 1m
        exp_alerts: []
      - alertname: LifecycleLateScan
        eval_time: 2m
        exp_alerts: []
      - alertname: LifecycleLateScan
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 4m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 3m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 5m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 3m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 6m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 7m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 8m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 3m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 9m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 3m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 10m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 3m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 11m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 3m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 12m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
          - exp_labels:
              severity: critical
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 3m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 13m
        exp_alerts:
          - exp_labels:
              severity: warning
            exp_annotations:
              zenko_service: backbeat-lifecycle-producer
              description: "Last lifecycle scan was performed more than 2m 0s ago."
              summary: "Lifecycle scan not executed in time"
      - alertname: LifecycleLateScan
        eval_time: 14m
        exp_alerts: []

  - name: LifecycleLatency
    interval: 10m
    input_series:
      - series: s3_lifecycle_latency_seconds_bucket{le="10", namespace="zenko", type="transition", location="us-east-1", job="artesca-data-backbeat-lifecycle-transition-processor-headless"}
        values: 100 200 300 305 310 315 320 325 330 430
      - series: s3_lifecycle_latency_seconds_bucket{le="50", namespace="zenko", type="transition", location="us-east-1", job="artesca-data-backbeat-lifecycle-transition-processor-headless"}
        values: 100 200 300 305 310 315 320 325 330 430
      - series: s3_lifecycle_latency_seconds_bucket{le="180", namespace="zenko", type="transition", location="us-east-1", job="artesca-data-backbeat-lifecycle-transition-processor-headless"}
        values: 100 200 300 400 500 600 605 610 615 620
      - series: s3_lifecycle_latency_seconds_bucket{le="240", namespace="zenko", type="transition", location="us-east-1", job="artesca-data-backbeat-lifecycle-transition-processor-headless"}
        values: 100 200 300 400 500 600 700 800 900 1000
      - series: s3_lifecycle_latency_seconds_bucket{le="+inf", namespace="zenko", type="transition", location="us-east-1", job="artesca-data-backbeat-lifecycle-transition-processor-headless"}
        values: 100 200 300 400 500 600 700 800 900 1000
    alert_rule_test:
      - alertname: LifecycleLatency
        eval_time: 10m
        exp_alerts: []
      - alertname: LifecycleLatency
        eval_time: 20m
        exp_alerts: []
      - alertname: LifecycleLatency
        eval_time: 30m
        exp_alerts:
          - exp_labels:
              severity: warning
              type: transition
              location: us-east-1
            exp_annotations:
              description: "Lifecycle latency for `transition` is above the warning threshold on location `us-east-1`."
              summary: "High lifecycle latency"
      - alertname: LifecycleLatency
        eval_time: 60m
        exp_alerts:
          - exp_labels:
              severity: warning
              type: transition
              location: us-east-1
            exp_annotations:
              description: "Lifecycle latency for `transition` is above the warning threshold on location `us-east-1`."
              summary: "High lifecycle latency"
          - exp_labels:
              severity: critical
              type: transition
              location: us-east-1
            exp_annotations:
              description: "Lifecycle latency for `transition` is above the critical threshold on location `us-east-1`."
              summary: "Very high lifecycle latency"

  - name: KafkaConsumerSlowTask
    interval: 1m
    input_series:
      - series: s3_backbeat_queue_slowTasks_count{namespace="zenko",job="artesca-data-backbeat-object-processor-headless"}
        values: 0 0 0 0 1 0 0 0 0 0
      - series: s3_backbeat_queue_slowTasks_count{namespace="zenko",job="artesca-data-backbeat-bucket-processor-headless"}
        values: 0 0 1 1 1 1 1 1 1 0
    alert_rule_test:
      - alertname: KafkaConsumerSlowTask
        eval_time: 1m
        exp_alerts: []
      - alertname: KafkaConsumerSlowTask
        eval_time: 3m
        exp_alerts: []
      - alertname: KafkaConsumerSlowTask
        eval_time: 5m
        exp_alerts: []
      - alertname: KafkaConsumerSlowTask
        eval_time: 6m
        exp_alerts: []
      - alertname: KafkaConsumerSlowTask
        eval_time: 7m
        exp_alerts:
          - exp_labels:
              severity: warning
              job: artesca-data-backbeat-bucket-processor-headless
            exp_annotations:
              description: >-
                Some tasks are taking too long to process in artesca-data-backbeat-bucket-processor-headless. This is not expected, and
                may be a sign that other components are not behaving nominally or may need to be scaled.

                If this alert lasts, it may mean the task is blocked, and that the consumer should be
                restarted.
              summary: Some Kafka messages are taking too long to process
      - alertname: KafkaConsumerSlowTask
        eval_time: 8m
        exp_alerts:
          - exp_labels:
              severity: warning
              job: artesca-data-backbeat-bucket-processor-headless
            exp_annotations:
              description: >-
                Some tasks are taking too long to process in artesca-data-backbeat-bucket-processor-headless. This is not expected, and
                may be a sign that other components are not behaving nominally or may need to be scaled.

                If this alert lasts, it may mean the task is blocked, and that the consumer should be
                restarted.
              summary: Some Kafka messages are taking too long to process
      - alertname: KafkaConsumerSlowTask
        eval_time: 9m
        exp_alerts: []

  - name: KafkaConsumerRebalance
    interval: 1m
    input_series:
      - series: s3_backbeat_queue_rebalance_total_count{namespace="zenko",job="artesca-data-backbeat-object-processor-headless",status="drained",pod="foo"}
        values: 1 2 _ _ stale
      - series: s3_backbeat_queue_rebalance_total_count{namespace="zenko",job="artesca-data-backbeat-object-processor-headless",status="timeout",pod="foo"}
        values: _ 1 _ _ stale
      - series: s3_backbeat_queue_rebalance_total_count{namespace="zenko",job="artesca-data-backbeat-object-processor-headless",status="drained",pod="bar"}
        values: _ _ 1 2 3 4 5 6 7
      - series: s3_backbeat_queue_rebalance_total_count{namespace="zenko",job="artesca-data-backbeat-object-processor-headless",status="timeout",pod="bar"}
        values: _ _ 0 0 0 0 0 0 0
    alert_rule_test:
      - alertname: KafkaConsumerRebalanceTimeout
        eval_time: 0m
        exp_alerts: []
      - alertname: KafkaConsumerRebalanceTimeout
        eval_time: 1m
        exp_alerts:
          - exp_labels:
              severity: critical
              pod: "foo"
            exp_annotations:
              summary: Kafka consumer has stopped consuming messages
              description: Kafka rebalance has timed out for pod `foo`, which indicates that the consumer is not working anymore, and should be restarted.
      - alertname: KafkaConsumerRebalanceTimeout
        eval_time: 2m
        exp_alerts:
          - exp_labels:
              severity: critical
              pod: "foo"
            exp_annotations:
              summary: Kafka consumer has stopped consuming messages
              description: Kafka rebalance has timed out for pod `foo`, which indicates that the consumer is not working anymore, and should be restarted.
      - alertname: KafkaConsumerRebalanceTimeout
        eval_time: 3m
        exp_alerts:
          - exp_labels:
              severity: critical
              pod: "foo"
            exp_annotations:
              summary: Kafka consumer has stopped consuming messages
              description: Kafka rebalance has timed out for pod `foo`, which indicates that the consumer is not working anymore, and should be restarted.
      - alertname: KafkaConsumerRebalanceTimeouts
        eval_time: 4m
        exp_alerts: []

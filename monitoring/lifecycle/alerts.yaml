x-inputs:
- name: namespace
  type: constant
- name: job_lifecycle_producer
  type: constant
- name: job_lifecycle_object_processor
  type: constant
- name: job_lifecycle_bucket_processor
  type: constant
- name: job_lifecycle_transition_processor
  type: constant
- name: job_sorbet_forwarder
  type: constant
- name: lifecycle_conductor_replicas
  type: constant
- name: lifecycle_bucket_replicas
  type: constant
- name: lifecycle_object_replicas
  type: constant
- name: lifecycle_transition_replicas
  type: constant
- name: lifecycle_latency_warning_threshold
  type: config
  value: 24*60*60 # 24h, in seconds
- name: lifecycle_latency_critical_threshold
  type: config
  value: 36*60*60 # 36h, in seconds

groups:
- name: LifecycleProducer
  rules:

  - alert: LifecycleProducerDown
    Expr: sum(up{namespace="${namespace}",job="${job_lifecycle_producer}"}) < ${lifecycle_conductor_replicas}
    For: "30s"
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-producer
      description: "Lifecycle producer pod has been down for 30 seconds"
      summary: "Lifecycle producer service is down"

  - alert: LifecycleLateScanWarning
    Expr: |
      (
        time()
        - max(s3_lifecycle_latest_batch_start_time{
             namespace="${namespace}", job="${job_lifecycle_producer}"
           }) / 1000
      ) / ${lifecycle_latency_warning_threshold} > 1
    Labels:
      severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-producer
      description: >-
        Last lifecycle scan was performed more than
        {{ ${lifecycle_latency_warning_threshold} | humanizeDuration }} ago.
      summary: "Lifecycle scan not executed in time"

  - alert: LifecycleLateScanCritical
    Expr: |
      (
        time()
        - max(s3_lifecycle_latest_batch_start_time{
             namespace="${namespace}", job="${job_lifecycle_producer}"
           }) / 1000
      ) / ${lifecycle_latency_critical_threshold} > 1
    Labels:
      severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-producer
      description: >-
        Last lifecycle scan was performed more than
        {{ ${lifecycle_latency_critical_threshold} | humanizeDuration }} ago.
      summary: "Lifecycle scan not executed in time"

- name: LifecycleBucketProcessor
  rules:

  - alert: LifecycleBucketProcessorDegraded
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_bucket_processor}"}) < ${lifecycle_bucket_replicas}
    For: "30s"
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-bucket-processor
      description: "Less than 100% of lifecycle bucket processors are up and healthy"
      summary: "Degraded lifecycle bucket processor"

  - alert: LifecycleBucketProcessorCritical
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_bucket_processor}"}) * 2 < ${lifecycle_bucket_replicas}
    For: "30s"
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-bucket-processor
      description: "Less than 50% of lifecycle bucket processors are up and healthy"
      summary: "Degraded lifecycle bucket processor"

  - alert: LifecycleBucketProcessorRequestWarning
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_bucket_processor}", status!="2xx", process="bucket"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_bucket_processor}", process="bucket"}[5m]))
        >= 0.03
    For: "5m"
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-bucket-processor
      description: "More than 3% of S3 requests by bucket processors resulting in errors"
      summary: "High rate of S3 request errors"

  - alert: LifecycleBucketProcessorRequestCritical
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_bucket_processor}", status!="2xx", process="bucket"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_bucket_processor}", process="bucket"}[5m]))
        >= 0.05
    For: "5m"
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-bucket-processor
      description: "More than 5% of S3 requests by bucket processors resulting in errors"
      summary: "Very high rate of S3 request errors"

- name: LifecycleObjectProcessor
  rules:

  - alert: LifecycleObjectProcessorDegraded
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_object_processor}"}) < ${lifecycle_object_replicas}
    For: "30s"
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-object-processor
      description: "Less than 100% of lifecycle object processors for expiration are up and healthy"
      summary: "Degraded lifecycle object processor"
  
  - alert: LifecycleObjectProcessorCritical
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_object_processor}"}) * 2 < ${lifecycle_object_replicas}
    For: "30s"
    Labels:
     severity: critical 
    Annotations:
      zenko_service: backbeat-lifecycle-object-processor
      description: "Less than 50% of lifecycle object processors for expiration are up and healthy"
      summary: "Degraded lifecycle object processor"
  
  - alert: LifecycleObjectProcessorRequestWarning
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_object_processor}", status!="2xx", process="expiration"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_object_processor}", process="expiration"}[5m]))
        >= 0.03
    For: "5m"
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-object-processor
      description: "More than 3% of S3 requests by object processors resulting in errors"
      summary: "High rate of S3 request errors"

  - alert: LifecycleObjectProcessorRequestCritical
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_object_processor}", status!="2xx", process="expiration"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_object_processor}", process="expiration"}[5m]))
        >= 0.05
    For: "5m"
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-object-processor
      description: "More than 5% of S3 requests by object processors resulting in errors"
      summary: "Very high rate of S3 request errors"

  - alert: LifecycleTransitionProcessorDegraded
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_transition_processor}"}) < ${lifecycle_transition_replicas}
    For: "30s"
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-transition-processor
      description: "Less than 100% of lifecycle transition processors are up and healthy"
      summary: "Degraded lifecycle transition processor"

  - alert: LifecycleTransitionProcessorCritical
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_transition_processor}"}) * 2 < ${lifecycle_transition_replicas}
    For: "30s"
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-transition-processor
      description: "Less than 50% of lifecycle transition processors are up and healthy"
      summary: "Degraded lifecycle transition processor"

  - alert: LifecycleTransitionProcessorRequestWarning
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_transition_processor}", status!="2xx"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_transition_processor}"}[5m]))
        >= 0.03
    For: "5m"
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-transition-processor
      description: "More than 3% of S3 requests by transition processors resulting in errors"
      summary: "High rate of S3 request errors"

  - alert: LifecycleTransitionProcessorRequestCritical
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_transition_processor}", status!="2xx"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_transition_processor}"}[5m]))
        >= 0.05
    For: "5m"
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-transition-processor
      description: "More than 5% of S3 requests by transition processors resulting in errors"
      summary: "Very high rate of S3 request errors"

- name: Kafka Messages
  rules:

  - alert: LifecycleKafkaBucketMessagesWarning
    Expr: |
      sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0)
        / (sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0)
        + sum(rate(s3_lifecycle_kafka_publish_success_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0))
        >= 0.03
    For: "5m"
    Labels:
     severity: warning
    Annotations:
      description: "More than 3% of Kafka messages failed to publish to the lifecycle bucket topic"
      summary: "High rate of failed messages to the bucket topic"

  - alert: LifecycleKafkaBucketMessagesCritical
    Expr: |
      sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0)
        / (sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0)
        + sum(rate(s3_lifecycle_kafka_publish_success_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0))
        >= 0.05
    For: "5m"
    Labels:
     severity: critical
    Annotations:
      description: "More than 5% of Kafka messages failed to publish to the lifecycle bucket topic"
      summary: "High rate of failed messages to the bucket topic"

  - alert: LifecycleKafkaObjectMessagesWarning
    Expr: |
      sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0)
        / (sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0)
        + sum(rate(s3_lifecycle_kafka_publish_success_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0))
        >= 0.03
    For: "5m"
    Labels:
     severity: warning
    Annotations:
      description: "More than 3% of Kafka messages failed to publish to the lifecycle object topic"
      summary: "High rate of failed messages to the object topic"

  - alert: LifecycleKafkaObjectMessagesCritical
    Expr: |
      sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0)
        / (sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0)
        + sum(rate(s3_lifecycle_kafka_publish_success_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0))
        >= 0.05
    For: "5m"
    Labels:
     severity: critical
    Annotations:
      description: "More than 5% of Kafka messages failed to publish to the lifecycle object topic"
      summary: "High rate of failed messages to the object topic"

  - alert: KafkaConsumerSlowTask
    Expr: |
      sum(s3_zenko_queue_slowTasks_count{namespace="${namespace}"}) by(job) > 0
    For: "5m"
    Labels:
     severity: warning
    Annotations:
      description: >-
        Some tasks are taking too long to process in {{ $labels.job }}. This is not expected, and
        may be a sign that other components are not behaving nominally or may need to be scaled.

        If this alert lasts, it may mean the task is blocked, and that the consumer should be
        restarted.
      summary: "Some Kafka messages are taking too long to process"

  - alert: KafkaConsumerRebalanceTimeout
    Expr: |
      sum(s3_zenko_queue_rebalance_total_count{namespace="${namespace}", status="timeout"}) by(pod) > 0
    Labels:
     severity: critical
    Annotations:
      description: >-
        Kafka rebalance has timed out for pod `{{ $labels.pod }}`, which indicates that the consumer
        is not working anymore, and should be restarted.
      summary: "Kafka consumer has stopped consuming messages"

x-inputs:
- name: namespace
  type: constant
- name: job_lifecycle_producer
  type: constant
- name: job_lifecycle_object_processor
  type: constant
- name: job_lifecycle_bucket_processor
  type: constant
- name: job_lifecycle_transition_processor
  type: constant
- name: job_sorbet_forwarder
  type: constant
- name: lifecycle_conductor_replicas
  type: constant
- name: lifecycle_bucket_replicas
  type: constant
- name: lifecycle_object_replicas
  type: constant
- name: lifecycle_transition_replicas
  type: constant
- name: lifecycle_latency_warning_threshold
  type: config
  value: 24*60*60 # 24h, in seconds
- name: lifecycle_latency_critical_threshold
  type: config
  value: 36*60*60 # 36h, in seconds

groups:
- name: LifecycleProducer
  rules:

  - alert: LifecycleProducerDown
    Expr: sum(up{namespace="${namespace}",job="${job_lifecycle_producer}"}) < ${lifecycle_conductor_replicas}
    For: "30s"
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-producer
      description: "Lifecycle producer pod has been down for 30 seconds"
      summary: "Lifecycle producer service is down"

  - alert: LifecycleLateScanWarning
    Expr: |
      (
        time() - (
          max(max_over_time(
            s3_lifecycle_latest_batch_start_time{
              namespace="${namespace}", job="${job_lifecycle_producer}"
            }[${lifecycle_latency_warning_threshold}s]
          )) / 1000
          > 0 or max(kube_service_created{namespace="${namespace}", service="${job_lifecycle_producer}"})
        )
      ) / ${lifecycle_latency_warning_threshold} > 1
    Labels:
      severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-producer
      description: >-
        Last lifecycle scan was performed more than
        {{ ${lifecycle_latency_warning_threshold} | humanizeDuration }} ago.
      summary: "Lifecycle scan not executed in time"

  - alert: LifecycleLateScanCritical
    Expr: |
      (
        time() - (
          max(max_over_time(
            s3_lifecycle_latest_batch_start_time{
              namespace="${namespace}", job="${job_lifecycle_producer}"
            }[${lifecycle_latency_warning_threshold}s]
          )) / 1000
          > 0 or max(kube_service_created{namespace="${namespace}", service="${job_lifecycle_producer}"})
        )
      ) / ${lifecycle_latency_critical_threshold} > 1
    Labels:
      severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-producer
      description: >-
        Last lifecycle scan was performed more than
        {{ ${lifecycle_latency_critical_threshold} | humanizeDuration }} ago.
      summary: "Lifecycle scan not executed in time"

- name: LifecycleBucketProcessor
  rules:

  - alert: LifecycleBucketProcessorDown
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_bucket_processor}"}) < ${lifecycle_bucket_replicas}
    For: "30s"
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-bucket-processor
      description: "Less than 100% of lifecycle bucket processors are up and healthy"
      summary: "Degraded lifecycle bucket processor"

  - alert: LifecycleBucketProcessorDown
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_bucket_processor}"}) * 2 < ${lifecycle_bucket_replicas}
    For: "30s"
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-bucket-processor
      description: "Less than 50% of lifecycle bucket processors are up and healthy"
      summary: "Degraded lifecycle bucket processor"

  - alert: LifecycleBucketProcessorRequest
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_bucket_processor}", status!~"^2..", process="bucket"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_bucket_processor}", process="bucket"}[5m]))
        >= 0.03
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-bucket-processor
      description: "More than 3% of S3 requests by bucket processors resulting in errors"
      summary: "High rate of S3 request errors"

  - alert: LifecycleBucketProcessorRequest
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_bucket_processor}", status!~"^2..", process="bucket"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_bucket_processor}", process="bucket"}[5m]))
        >= 0.05
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-bucket-processor
      description: "More than 5% of S3 requests by bucket processors resulting in errors"
      summary: "Very high rate of S3 request errors"

- name: LifecycleObjectProcessor
  rules:

  - alert: LifecycleObjectProcessorDown
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_object_processor}"}) < ${lifecycle_object_replicas}
    For: "30s"
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-object-processor
      description: "Less than 100% of lifecycle object processors for expiration are up and healthy"
      summary: "Degraded lifecycle object processor"
  
  - alert: LifecycleObjectProcessorDown
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_object_processor}"}) * 2 < ${lifecycle_object_replicas}
    For: "30s"
    Labels:
     severity: critical 
    Annotations:
      zenko_service: backbeat-lifecycle-object-processor
      description: "Less than 50% of lifecycle object processors for expiration are up and healthy"
      summary: "Degraded lifecycle object processor"
  
  - alert: LifecycleObjectProcessorRequest
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_object_processor}", status!~"^2..", process="expiration"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_object_processor}", process="expiration"}[5m]))
        >= 0.03
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-object-processor
      description: "More than 3% of S3 requests by object processors resulting in errors"
      summary: "High rate of S3 request errors"

  - alert: LifecycleObjectProcessorRequest
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_object_processor}", status!~"^2..", process="expiration"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_object_processor}", process="expiration"}[5m]))
        >= 0.05
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-object-processor
      description: "More than 5% of S3 requests by object processors resulting in errors"
      summary: "Very high rate of S3 request errors"

  - alert: LifecycleTransitionProcessorDegraded
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_transition_processor}"}) < ${lifecycle_transition_replicas}
    For: "30s"
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-transition-processor
      description: "Less than 100% of lifecycle transition processors are up and healthy"
      summary: "Degraded lifecycle transition processor"

  - alert: LifecycleTransitionProcessorCritical
    Expr: sum(up{namespace="${namespace}", job="${job_lifecycle_transition_processor}"}) * 2 < ${lifecycle_transition_replicas}
    For: "30s"
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-transition-processor
      description: "Less than 50% of lifecycle transition processors are up and healthy"
      summary: "Degraded lifecycle transition processor"

  - alert: LifecycleTransitionProcessorRequestWarning
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_transition_processor}", status!="2xx"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_transition_processor}"}[5m]))
        >= 0.03
    For: "5m"
    Labels:
     severity: warning
    Annotations:
      zenko_service: backbeat-lifecycle-transition-processor
      description: "More than 3% of S3 requests by transition processors resulting in errors"
      summary: "High rate of S3 request errors"

  - alert: LifecycleTransitionProcessorRequestCritical
    Expr: |
      sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_transition_processor}", status!="2xx"}[5m]))
        / sum(rate(s3_lifecycle_s3_operations_total{namespace="${namespace}", job="${job_lifecycle_transition_processor}"}[5m]))
        >= 0.05
    For: "5m"
    Labels:
     severity: critical
    Annotations:
      zenko_service: backbeat-lifecycle-transition-processor
      description: "More than 5% of S3 requests by transition processors resulting in errors"
      summary: "Very high rate of S3 request errors"

  - alert: LifecycleLatencyWarning
    Expr: |
      histogram_quantile(0.95,
        sum(rate(
          s3_lifecycle_latency_seconds_bucket{
            namespace="${namespace}",job=~"${job_lifecycle_object_processor}|${job_lifecycle_transition_processor}|${job_sorbet_forwarder}.*"
          }[10m]
        )) by(le, type, location)
      ) / ${lifecycle_latency_warning_threshold} > 1
    Labels:
      severity: warning
    Annotations:
      description: >-
        Lifecycle latency for `{{ $labels.type }}` is above the warning threshold on location
        `{{ $labels.location }}`.
      summary: "High lifecycle latency"

  - alert: LifecycleLatencyCritical
    Expr: |
      histogram_quantile(0.95,
        sum(rate(
          s3_lifecycle_latency_seconds_bucket{
            namespace="${namespace}", job=~"${job_lifecycle_object_processor}|${job_lifecycle_transition_processor}|${job_sorbet_forwarder}.*"
          }[10m]
        )) by(le, type, location)
      ) / ${lifecycle_latency_critical_threshold} > 1
    Labels:
      severity: critical
    Annotations:
      description: >-
        Lifecycle latency for `{{ $labels.type }}` is above the critical threshold on location
        `{{ $labels.location }}`.
      summary: "Very high lifecycle latency"

- name: Kafka Messages
  rules:

  - alert: LifecycleKafkaBucketMessages
    Expr: |
      sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0)
        / (sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0)
        + sum(rate(s3_lifecycle_kafka_publish_success_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0))
        >= 0.03
    For: "5m"
    Labels:
     severity: warning
    Annotations:
      description: "More than 3% of Kafka messages failed to publish to the lifecycle bucket topic"
      summary: "High rate of failed messages to the bucket topic"

  - alert: LifecycleKafkaBucketMessages
    Expr: |
      sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0)
        / (sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0)
        + sum(rate(s3_lifecycle_kafka_publish_success_total{namespace="${namespace}",op="BucketTopic"}[5m]) > 0))
        >= 0.05
    For: "5m"
    Labels:
     severity: critical
    Annotations:
      description: "More than 5% of Kafka messages failed to publish to the lifecycle bucket topic"
      summary: "High rate of failed messages to the bucket topic"

  - alert: LifecycleKafkaObjectMessages
    Expr: |
      sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0)
        / (sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0)
        + sum(rate(s3_lifecycle_kafka_publish_success_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0))
        >= 0.03
    For: "5m"
    Labels:
     severity: warning
    Annotations:
      description: "More than 3% of Kafka messages failed to publish to the lifecycle object topic"
      summary: "High rate of failed messages to the object topic"

  - alert: LifecycleKafkaObjectMessages
    Expr: |
      sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0)
        / (sum(rate(s3_lifecycle_kafka_publish_error_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0)
        + sum(rate(s3_lifecycle_kafka_publish_success_total{namespace="${namespace}",op="ObjectTopic"}[5m]) > 0))
        >= 0.05
    For: "5m"
    Labels:
     severity: critical
    Annotations:
      description: "More than 5% of Kafka messages failed to publish to the lifecycle object topic"
      summary: "High rate of failed messages to the object topic"

  - alert: KafkaConsumerSlowTask
    Expr: |
      sum(s3_backbeat_queue_slowTasks_count{namespace="${namespace}"}) by(job) > 0
    For: "5m"
    Labels:
     severity: warning
    Annotations:
      description: >-
        Some tasks are taking too long to process in {{ $labels.job }}. This is not expected, and
        may be a sign that other components are not behaving nominally or may need to be scaled.

        If this alert lasts, it may mean the task is blocked, and that the consumer should be
        restarted.
      summary: "Some Kafka messages are taking too long to process"

  - alert: KafkaConsumerRebalanceTimeout
    Expr: |
      sum(s3_backbeat_queue_rebalance_total_count{namespace="${namespace}", status="timeout"}) by(pod) > 0
    Labels:
     severity: critical
    Annotations:
      description: >-
        Kafka rebalance has timed out for pod `{{ $labels.pod }}`, which indicates that the consumer
        is not working anymore, and should be restarted.
      summary: "Kafka consumer has stopped consuming messages"
